"""
–ö–ª–∞—Å—Å NewsDigest ‚Äî –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ NewsData.io —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
–°–æ–±–ª—é–¥–∞–µ—Ç –ª–∏–º–∏—Ç—ã: 200 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å (Free tier).

API: https://newsdata.io/
–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://newsdata.io/docs
"""
import json
import asyncio
import logging
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field

import aiohttp

from config import NEWSDATA_API_KEY

logger = logging.getLogger(__name__)


# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
NEWSDATA_BASE = "https://newsdata.io/api/1"

# –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ (—Å–µ–∫—É–Ω–¥—ã)
CACHE_TTL = {
    "headlines_ru": 30 * 60,       # 30 –º–∏–Ω—É—Ç
    "headlines_ru_world": 30 * 60, # 30 –º–∏–Ω—É—Ç
    "headlines_ru_tech": 30 * 60,  # 30 –º–∏–Ω—É—Ç
    "headlines_ru_business": 30 * 60,  # 30 –º–∏–Ω—É—Ç
    "headlines_ru_science": 30 * 60,   # 30 –º–∏–Ω—É—Ç
}

REQUEST_TIMEOUT = aiohttp.ClientTimeout(total=15)

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π NewsData.io
NEWS_CATEGORIES = {
    "world": "üåç –ú–∏—Ä",
    "technology": "üíª –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏",
    "business": "üíº –ë–∏–∑–Ω–µ—Å",
    "science": "üî¨ –ù–∞—É–∫–∞",
    "health": "üè• –ó–¥–æ—Ä–æ–≤—å–µ",
    "sports": "‚öΩ –°–ø–æ—Ä—Ç",
    "entertainment": "üé¨ –†–∞–∑–≤–ª–µ—á–µ–Ω–∏—è",
    "politics": "üèõÔ∏è –ü–æ–ª–∏—Ç–∏–∫–∞",
    "top": "üì∞ –ì–ª–∞–≤–Ω–æ–µ",
}

# –Ø–∑—ã–∫–∏
NEWS_LANGUAGES = {
    "ru": "üá∑üá∫ –†—É—Å—Å–∫–∏–π",
    "en": "üá∫üá∏ English",
}


@dataclass
class CacheEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫—ç—à–µ"""
    data: Any
    fetched_at: float
    is_stale: bool = False
    api_calls: int = 0


@dataclass
class APIMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API"""
    total_calls: int = 0
    daily_calls: int = 0
    last_reset_date: str = ""
    
    def reset_if_new_day(self):
        """–°–±—Ä–æ—Å –¥–Ω–µ–≤–Ω–æ–≥–æ —Å—á—ë—Ç—á–∏–∫–∞"""
        today = datetime.now().strftime("%Y-%m-%d")
        if self.last_reset_date != today:
            self.daily_calls = 0
            self.last_reset_date = today
            logger.info(f"NewsDigest: Daily counter reset for {today}")


class NewsDigest:
    """
    –ù–æ–≤–æ—Å—Ç–∏ –∏–∑ NewsData.io —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ö—ç—à –≤ –ø–∞–º—è—Ç–∏ + JSON-—Ñ–∞–π–ª (fallback)
    - –õ–∏–º–∏—Ç 200 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å (Free tier)
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    - Graceful degradation –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    """
    
    def __init__(self, cache_path: Path, api_key: str = None):
        self.cache_path = cache_path
        self.api_key = api_key or NEWSDATA_API_KEY
        self._cache: Dict[str, CacheEntry] = {}
        self._session: Optional[aiohttp.ClientSession] = None
        self._lock = asyncio.Lock()
        self._metrics = APIMetrics()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –∏–∑ —Ñ–∞–π–ª–∞
        self._load_cache_from_file()
    
    # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
    
    async def _get_session(self) -> aiohttp.ClientSession:
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession(
                timeout=REQUEST_TIMEOUT,
                connector=aiohttp.TCPConnector(limit=5)
            )
        return self._session
    
    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()
            logger.debug("NewsDigest: HTTP session closed")
    
    # === –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ===
    
    def _load_cache_from_file(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
        if not self.cache_path.exists():
            return
        
        try:
            with open(self.cache_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            for key, entry in data.get("cache", {}).items():
                if isinstance(entry, dict) and "data" in entry:
                    self._cache[key] = CacheEntry(
                        data=entry["data"],
                        fetched_at=entry.get("fetched_at", 0),
                        is_stale=entry.get("is_stale", False),
                        api_calls=entry.get("api_calls", 0)
                    )
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            if "metrics" in data:
                self._metrics.total_calls = data["metrics"].get("total_calls", 0)
                self._metrics.daily_calls = data["metrics"].get("daily_calls", 0)
                self._metrics.last_reset_date = data["metrics"].get("last_reset_date", "")
            
            logger.info(f"NewsDigest: Loaded {len(self._cache)} cache entries")
            
        except Exception as e:
            logger.warning(f"NewsDigest: Failed to load cache: {e}")
    
    def _save_cache_to_file(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –≤ JSON-—Ñ–∞–π–ª"""
        try:
            data = {
                "cache": {
                    key: {
                        "data": entry.data,
                        "fetched_at": entry.fetched_at,
                        "is_stale": entry.is_stale,
                        "api_calls": entry.api_calls
                    }
                    for key, entry in self._cache.items()
                },
                "metrics": {
                    "total_calls": self._metrics.total_calls,
                    "daily_calls": self._metrics.daily_calls,
                    "last_reset_date": self._metrics.last_reset_date
                }
            }
            
            temp_path = self.cache_path.with_suffix(".tmp")
            with open(temp_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            temp_path.replace(self.cache_path)
            
        except Exception as e:
            logger.error(f"NewsDigest: Failed to save cache: {e}")
    
    def _is_cache_valid(self, key: str) -> bool:
        if key not in self._cache:
            return False
        
        entry = self._cache[key]
        ttl = CACHE_TTL.get(key, 30 * 60)
        age = time.time() - entry.fetched_at
        
        return age < ttl
    
    def _get_cached(self, key: str) -> Optional[CacheEntry]:
        return self._cache.get(key)
    
    def _set_cached(self, key: str, data: Any, is_stale: bool = False):
        entry = CacheEntry(
            data=data,
            fetched_at=time.time(),
            is_stale=is_stale,
            api_calls=self._cache.get(key, CacheEntry(data=None, fetched_at=0)).api_calls + 1
        )
        self._cache[key] = entry
        self._save_cache_to_file()
    
    # === API –ó–ê–ü–†–û–°–´ ===
    
    async def _fetch_newsdata(self, endpoint: str, params: Dict = None) -> Optional[Dict]:
        """
        –ó–∞–ø—Ä–æ—Å –∫ NewsData.io —Å —É—á—ë—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤.
        –õ–∏–º–∏—Ç: 200 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å –Ω–∞ Free tier.
        """
        self._metrics.reset_if_new_day()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–Ω–µ–≤–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å 10 –∑–∞–ø—Ä–æ—Å–æ–≤)
        if self._metrics.daily_calls >= 190:
            logger.warning("NewsDigest: Daily API limit approaching (190/200)")
            return None
        
        if not self.api_key:
            logger.warning("NewsDigest: API key not configured")
            return None
        
        try:
            session = await self._get_session()
            url = f"{NEWSDATA_BASE}{endpoint}"
            
            # –î–æ–±–∞–≤–ª—è–µ–º API key
            params = params or {}
            params["apikey"] = self.api_key
            
            self._metrics.total_calls += 1
            self._metrics.daily_calls += 1
            
            logger.info(
                f"NewsDigest: API call #{self._metrics.daily_calls}/200 today ‚Üí {endpoint}"
            )
            
            async with session.get(url, params=params) as resp:
                if resp.status == 429:
                    logger.warning("NewsDigest: Rate limit hit (429)")
                    return None
                
                if resp.status == 401:
                    logger.error("NewsDigest: Invalid API key (401)")
                    return None
                
                resp.raise_for_status()
                data = await resp.json()
                
                if data.get("status") != "success":
                    logger.warning(f"NewsDigest: API error: {data.get('results', {}).get('message', 'Unknown error')}")
                    return None
                
                return data
                
        except aiohttp.ClientError as e:
            logger.error(f"NewsDigest: HTTP error: {e}")
            return None
        except Exception as e:
            logger.error(f"NewsDigest: Unexpected error: {e}")
            return None
    
    # === –ú–ï–¢–û–î–´ –î–ê–ù–ù–´–• ===
    
    async def get_latest_news(
        self,
        language: str = "ru",
        category: str = None,
        country: str = None,
        page_size: int = 10
    ) -> Optional[List[Dict]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ NewsData.io.
        
        Args:
            language: –ö–æ–¥ —è–∑—ã–∫–∞ (ru, en)
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è (world, technology, business, etc.)
            country: –ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (ru, us, gb, etc.)
            page_size: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π (–º–∞–∫—Å. 10 –Ω–∞ Free tier)
        """
        cache_key = f"headlines_{language}"
        if category:
            cache_key = f"headlines_{language}_{category}"
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –≤–∞–ª–∏–¥–µ–Ω
        if self._is_cache_valid(cache_key):
            entry = self._get_cached(cache_key)
            logger.debug(f"NewsDigest: Returning cached {cache_key}")
            return entry.data
        
        async with self._lock:
            if self._is_cache_valid(cache_key):
                return self._get_cached(cache_key).data
            
            params = {
                "language": language,
            }
            
            if category:
                params["category"] = category
            if country:
                params["country"] = country
            
            data = await self._fetch_newsdata("/latest", params)
            
            if data and data.get("results"):
                articles = self._normalize_articles(data["results"])
                self._set_cached(cache_key, articles, is_stale=False)
                return articles
            
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à
            entry = self._get_cached(cache_key)
            if entry and entry.data:
                logger.warning(f"NewsDigest: Returning stale {cache_key}")
                entry.is_stale = True
                return entry.data
            
            return None
    
    def _normalize_articles(self, articles: List[Dict]) -> List[Dict]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–µ–π –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        normalized = []
        
        for article in articles:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏ –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ URL
            if not article.get("title") or not article.get("link"):
                continue
            
            normalized.append({
                "title": article.get("title", ""),
                "description": article.get("description", "") or article.get("content", ""),
                "url": article.get("link", ""),
                "source": article.get("source_id", "–ò—Å—Ç–æ—á–Ω–∏–∫"),
                "author": article.get("creator", [""])[0] if article.get("creator") else "",
                "published_at": article.get("pubDate", ""),
                "image_url": article.get("image_url", ""),
                "category": article.get("category", [""])[0] if article.get("category") else "",
            })
        
        return normalized
    
    # === –°–í–ï–ñ–ò–ï –î–ê–ù–ù–´–ï ===
    
    async def refresh_all(self) -> Dict[str, bool]:
        """
        –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ª–µ–Ω—Ç.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é.
        """
        logger.info("NewsDigest: Starting refresh")
        
        results = {}
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä—É—Å—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        tasks = [
            ("ru_top", self.get_latest_news(language="ru", category="top")),
            ("ru_world", self.get_latest_news(language="ru", category="world")),
            ("ru_technology", self.get_latest_news(language="ru", category="technology")),
            ("ru_business", self.get_latest_news(language="ru", category="business")),
            ("ru_science", self.get_latest_news(language="ru", category="science")),
        ]
        
        for name, task in tasks:
            try:
                result = await task
                results[name] = result is not None
            except Exception as e:
                logger.error(f"NewsDigest: Error refreshing {name}: {e}")
                results[name] = False
        
        success = sum(results.values())
        logger.info(f"NewsDigest: Refresh complete ({success}/{len(results)} sources)")
        
        return results
    
    # === –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï ===
    
    def get_news_digest(
        self,
        language: str = "ru",
        category: str = "top",
        max_items: int = 5
    ) -> str:
        """
        –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è Telegram.
        –ë–ï–ó –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API ‚Äî —Ç–æ–ª—å–∫–æ –∏–∑ –∫—ç—à–∞!
        
        Args:
            language: –ö–æ–¥ —è–∑—ã–∫–∞
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è
            max_items: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        cache_key = f"headlines_{language}"
        if category:
            cache_key = f"headlines_{language}_{category}"
        
        entry = self._cache.get(cache_key)
        
        if not entry or not entry.data:
            logger.warning(f"NewsDigest: No cached data for {cache_key}, available keys: {list(self._cache.keys())}")
            return "üì∞ <b>–ù–æ–≤–æ—Å—Ç–∏</b>\n\n‚ùå –î–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
        
        lang_name = NEWS_LANGUAGES.get(language, language.upper())
        category_name = NEWS_CATEGORIES.get(category, category)
        
        lines = [f"üì∞ <b>–ù–æ–≤–æ—Å—Ç–∏</b> ‚Ä¢ {lang_name} ‚Ä¢ {category_name}"]
        
        if entry.is_stale:
            lines.append("‚ö†Ô∏è <i>–î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º–∏</i>")
        
        lines.append("")
        
        for i, article in enumerate(entry.data[:max_items], 1):
            title = article.get("title", "")
            source = article.get("source", "–ò—Å—Ç–æ—á–Ω–∏–∫")
            url = article.get("url", "#")
            
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML
            import html as html_module
            title = html_module.escape(title[:100] + "..." if len(title) > 100 else title)
            source = html_module.escape(source)
            
            lines.append(f"{i}. <a href=\"{url}\">{title}</a>")
            lines.append(f"   <i>{source}</i>\n")
        
        # –í—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if entry.fetched_at:
            age = int(time.time() - entry.fetched_at)
            if age < 60:
                age_str = f"{age}—Å –Ω–∞–∑–∞–¥"
            elif age < 3600:
                age_str = f"{age // 60}–º–∏–Ω –Ω–∞–∑–∞–¥"
            else:
                age_str = f"{age // 3600}—á –Ω–∞–∑–∞–¥"
            lines.append(f"üïê <i>–û–±–Ω–æ–≤–ª–µ–Ω–æ: {age_str}</i>")
        
        return "\n".join(lines)
    
    def get_combined_digest(self, max_per_category: int = 3) -> str:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.
        –ë–ï–ó –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API ‚Äî —Ç–æ–ª—å–∫–æ –∏–∑ –∫—ç—à–∞!
        """
        lines = ["üì∞ <b>–ù–æ–≤–æ—Å—Ç–∏ –¥–Ω—è</b>\n"]
        
        has_any = False
        
        # –ì–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        top_entry = self._cache.get("headlines_ru_top")
        if not top_entry or not top_entry.data:
            top_entry = self._cache.get("headlines_ru")
        
        if top_entry and top_entry.data:
            has_any = True
            lines.append("üì∞ <b>–ì–ª–∞–≤–Ω–æ–µ:</b>")
            for article in top_entry.data[:max_per_category]:
                title = article.get("title", "")[:80]
                url = article.get("url", "#")
                import html as html_module
                title = html_module.escape(title)
                lines.append(f" ‚Ä¢ <a href=\"{url}\">{title}</a>")
            lines.append("")
        
        # –ú–∏—Ä–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        world_entry = self._cache.get("headlines_ru_world")
        if world_entry and world_entry.data:
            has_any = True
            lines.append("üåç <b>–í –º–∏—Ä–µ:</b>")
            for article in world_entry.data[:max_per_category]:
                title = article.get("title", "")[:80]
                url = article.get("url", "#")
                import html as html_module
                title = html_module.escape(title)
                lines.append(f" ‚Ä¢ <a href=\"{url}\">{title}</a>")
            lines.append("")
        
        # –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        tech_entry = self._cache.get("headlines_ru_technology")
        if tech_entry and tech_entry.data:
            has_any = True
            lines.append("üíª <b>–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:</b>")
            for article in tech_entry.data[:max_per_category]:
                title = article.get("title", "")[:80]
                url = article.get("url", "#")
                import html as html_module
                title = html_module.escape(title)
                lines.append(f" ‚Ä¢ <a href=\"{url}\">{title}</a>")
            lines.append("")
        
        # –ë–∏–∑–Ω–µ—Å
        biz_entry = self._cache.get("headlines_ru_business")
        if biz_entry and biz_entry.data:
            has_any = True
            lines.append("üíº <b>–ë–∏–∑–Ω–µ—Å:</b>")
            for article in biz_entry.data[:max_per_category]:
                title = article.get("title", "")[:80]
                url = article.get("url", "#")
                import html as html_module
                title = html_module.escape(title)
                lines.append(f" ‚Ä¢ <a href=\"{url}\">{title}</a>")
            lines.append("")
        
        if not has_any:
            logger.warning(f"NewsDigest: No cached data for combined digest, available keys: {list(self._cache.keys())}")
            return "üì∞ <b>–ù–æ–≤–æ—Å—Ç–∏</b>\n\n‚ùå –î–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        lines.append(f"üìä API: {self._metrics.daily_calls}/200 –∑–∞–ø—Ä–æ—Å–æ–≤ —Å–µ–≥–æ–¥–Ω—è")
        
        return "\n".join(lines)
    
    # === –ú–ï–¢–†–ò–ö–ò ===
    
    def get_metrics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API"""
        return {
            "total_calls": self._metrics.total_calls,
            "daily_calls": self._metrics.daily_calls,
            "daily_limit": 200,
            "remaining": 200 - self._metrics.daily_calls,
            "cache_entries": len(self._cache),
            "cache_status": {
                key: {
                    "valid": self._is_cache_valid(key),
                    "age_seconds": int(time.time() - entry.fetched_at) if entry else None,
                    "is_stale": entry.is_stale if entry else None,
                    "articles_count": len(entry.data) if entry and entry.data else 0
                }
                for key, entry in self._cache.items()
            }
        }
